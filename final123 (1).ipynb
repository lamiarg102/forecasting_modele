{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8970575,"sourceType":"datasetVersion","datasetId":5400558}],"dockerImageVersionId":30747,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**importer les bibliothèques**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport statsmodels.api as sm\nimport warnings\nwarnings.filterwarnings('ignore')\nimport datetime\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.stattools import adfuller","metadata":{"execution":{"iopub.status.busy":"2024-08-04T00:03:49.217489Z","iopub.execute_input":"2024-08-04T00:03:49.217765Z","iopub.status.idle":"2024-08-04T00:03:53.258210Z","shell.execute_reply.started":"2024-08-04T00:03:49.217742Z","shell.execute_reply":"2024-08-04T00:03:53.257203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**import les données**","metadata":{}},{"cell_type":"code","source":"# Load the dataset (adjust the path to your dataset)\ndata = pd.read_csv('/kaggle/input/datazero2/output_modified.csv')\n\n# Check the columns in the dataset\nprint(data.columns)","metadata":{"execution":{"iopub.status.busy":"2024-08-04T00:04:05.584250Z","iopub.execute_input":"2024-08-04T00:04:05.584613Z","iopub.status.idle":"2024-08-04T00:04:07.380225Z","shell.execute_reply.started":"2024-08-04T00:04:05.584588Z","shell.execute_reply":"2024-08-04T00:04:07.379295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Preparartion des données(1)**","metadata":{}},{"cell_type":"markdown","source":"\"Vérification et suppression des valeurs manquantes dans les données\"","metadata":{}},{"cell_type":"code","source":"# Check and drop missing values\nprint(data.isnull().sum())\ndata.dropna(inplace=True)\nprint(data.info())","metadata":{"execution":{"iopub.status.busy":"2024-08-04T00:04:10.492808Z","iopub.execute_input":"2024-08-04T00:04:10.493759Z","iopub.status.idle":"2024-08-04T00:04:10.909161Z","shell.execute_reply.started":"2024-08-04T00:04:10.493717Z","shell.execute_reply":"2024-08-04T00:04:10.906429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert 'time' column to datetime and set as index\ndata['time'] = pd.to_datetime(data['time'], errors='coerce')\ndata.dropna(subset=['time'], inplace=True)\ndata.set_index('time', inplace=True)\n# Ensure the index is a DatetimeIndex\ndata.index = pd.to_datetime(data.index)","metadata":{"execution":{"iopub.status.busy":"2024-08-04T00:04:15.028503Z","iopub.execute_input":"2024-08-04T00:04:15.028864Z","iopub.status.idle":"2024-08-04T00:04:16.812182Z","shell.execute_reply.started":"2024-08-04T00:04:15.028834Z","shell.execute_reply":"2024-08-04T00:04:16.811116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Proportion de Valeurs Nulles par Colonne","metadata":{}},{"cell_type":"code","source":"data.isnull().mean()","metadata":{"execution":{"iopub.status.busy":"2024-08-03T17:41:15.060271Z","iopub.execute_input":"2024-08-03T17:41:15.061153Z","iopub.status.idle":"2024-08-03T17:41:15.076121Z","shell.execute_reply.started":"2024-08-03T17:41:15.061116Z","shell.execute_reply":"2024-08-03T17:41:15.075129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.index","metadata":{"execution":{"iopub.status.busy":"2024-08-03T17:41:15.077401Z","iopub.execute_input":"2024-08-03T17:41:15.077700Z","iopub.status.idle":"2024-08-03T17:41:15.084297Z","shell.execute_reply.started":"2024-08-03T17:41:15.077674Z","shell.execute_reply":"2024-08-03T17:41:15.083425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.index[:3]","metadata":{"execution":{"iopub.status.busy":"2024-08-03T17:41:15.085484Z","iopub.execute_input":"2024-08-03T17:41:15.085817Z","iopub.status.idle":"2024-08-03T17:41:15.095559Z","shell.execute_reply.started":"2024-08-03T17:41:15.085787Z","shell.execute_reply":"2024-08-03T17:41:15.094588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\"Premiers Trois Index du DataFrame\"\n\n\"Année des Premiers Trois Index du DataFrame\"\n\n\"Nom du Jour des 10 000 Premiers Index du DataFrame\"","metadata":{}},{"cell_type":"code","source":"data.index[:3].year","metadata":{"execution":{"iopub.status.busy":"2024-08-03T17:41:15.096703Z","iopub.execute_input":"2024-08-03T17:41:15.096954Z","iopub.status.idle":"2024-08-03T17:41:15.106622Z","shell.execute_reply.started":"2024-08-03T17:41:15.096933Z","shell.execute_reply":"2024-08-03T17:41:15.105552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.index[:10].month","metadata":{"execution":{"iopub.status.busy":"2024-08-03T17:41:15.107794Z","iopub.execute_input":"2024-08-03T17:41:15.108128Z","iopub.status.idle":"2024-08-03T17:41:15.118872Z","shell.execute_reply.started":"2024-08-03T17:41:15.108078Z","shell.execute_reply":"2024-08-03T17:41:15.118080Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.index[:20000].day","metadata":{"execution":{"iopub.status.busy":"2024-08-03T17:41:15.120161Z","iopub.execute_input":"2024-08-03T17:41:15.120513Z","iopub.status.idle":"2024-08-03T17:41:15.133292Z","shell.execute_reply.started":"2024-08-03T17:41:15.120480Z","shell.execute_reply":"2024-08-03T17:41:15.132226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.index[:5000].day_name()","metadata":{"execution":{"iopub.status.busy":"2024-08-03T17:41:15.137196Z","iopub.execute_input":"2024-08-03T17:41:15.137464Z","iopub.status.idle":"2024-08-03T17:41:15.145160Z","shell.execute_reply.started":"2024-08-03T17:41:15.137441Z","shell.execute_reply":"2024-08-03T17:41:15.144260Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Création de colonnes (features) relatives à la date : ","metadata":{}},{"cell_type":"code","source":"data['Year'] = data.index.year\ndata['Month'] = data.index.month\ndata['day_name'] = data.index.day_name()\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-04T00:04:26.356816Z","iopub.execute_input":"2024-08-04T00:04:26.357170Z","iopub.status.idle":"2024-08-04T00:04:26.679929Z","shell.execute_reply.started":"2024-08-04T00:04:26.357141Z","shell.execute_reply":"2024-08-04T00:04:26.678982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-03T17:41:15.494189Z","iopub.execute_input":"2024-08-03T17:41:15.494559Z","iopub.status.idle":"2024-08-03T17:41:15.509660Z","shell.execute_reply.started":"2024-08-03T17:41:15.494525Z","shell.execute_reply":"2024-08-03T17:41:15.508566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.Month.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-08-03T17:41:15.510883Z","iopub.execute_input":"2024-08-03T17:41:15.511201Z","iopub.status.idle":"2024-08-03T17:41:15.528560Z","shell.execute_reply.started":"2024-08-03T17:41:15.511177Z","shell.execute_reply":"2024-08-03T17:41:15.527613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.day_name.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-08-03T17:41:15.529876Z","iopub.execute_input":"2024-08-03T17:41:15.530196Z","iopub.status.idle":"2024-08-03T17:41:15.710931Z","shell.execute_reply.started":"2024-08-03T17:41:15.530172Z","shell.execute_reply":"2024-08-03T17:41:15.709835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['is_weekend'] = data.day_name.isin([\"Sunday\", \"Saturday\"])","metadata":{"execution":{"iopub.status.busy":"2024-08-03T17:41:15.712187Z","iopub.execute_input":"2024-08-03T17:41:15.712477Z","iopub.status.idle":"2024-08-03T17:41:15.761782Z","shell.execute_reply.started":"2024-08-03T17:41:15.712449Z","shell.execute_reply":"2024-08-03T17:41:15.761008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### Vérification de la Présence d'une Valeur dans un Ensemble de Possibilités en Python\nx = 2\n\n# Utilisation de 'or' avec les conditions\nif x == 5 or x == 9 or x == 12:\n    pass  # Le 'pass' signifie qu'aucune action n'est effectuée si la condition est vraie\n\n# Utilisation de 'in' avec une liste\nif x in [5, 9, 12]:\n    pass  # Le 'pass' signifie qu'aucune action n'est effectuée si la condition est vraie","metadata":{"execution":{"iopub.status.busy":"2024-08-03T17:41:15.762808Z","iopub.execute_input":"2024-08-03T17:41:15.763087Z","iopub.status.idle":"2024-08-03T17:41:15.772185Z","shell.execute_reply.started":"2024-08-03T17:41:15.763037Z","shell.execute_reply":"2024-08-03T17:41:15.771286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(data.day_name == 'Saturday') &   (data.day_name == 'Sunday')","metadata":{"execution":{"iopub.status.busy":"2024-08-03T17:41:15.773309Z","iopub.execute_input":"2024-08-03T17:41:15.773560Z","iopub.status.idle":"2024-08-03T17:41:16.093568Z","shell.execute_reply.started":"2024-08-03T17:41:15.773538Z","shell.execute_reply":"2024-08-03T17:41:16.092445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-03T17:41:16.095054Z","iopub.execute_input":"2024-08-03T17:41:16.095383Z","iopub.status.idle":"2024-08-03T17:41:16.111110Z","shell.execute_reply.started":"2024-08-03T17:41:16.095358Z","shell.execute_reply":"2024-08-03T17:41:16.110102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Quel est le jour avec le plus de consommation de power en global?","metadata":{}},{"cell_type":"code","source":"# Group by day_name and summarize power consumption\npower_columns = ['power', 'power1', 'power2', 'power3']\nfor col in power_columns:\n    print(data.groupby('day_name').sum()[col].sort_values())","metadata":{"execution":{"iopub.status.busy":"2024-08-04T00:04:33.516778Z","iopub.execute_input":"2024-08-04T00:04:33.517392Z","iopub.status.idle":"2024-08-04T00:04:33.968745Z","shell.execute_reply.started":"2024-08-04T00:04:33.517360Z","shell.execute_reply":"2024-08-04T00:04:33.967267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\"Après les quatre résultats, nous constatons que le jour ayant toujours la plus grande consommation de puissance est le mardi (Tuesday).\"","metadata":{}},{"cell_type":"markdown","source":"**Analyse et exploratoire des données**\n\ntendance de puissance  durant une heure","metadata":{}},{"cell_type":"code","source":"# Filtrer les données pour une heure\ndata_1hour = data.loc[data.index < data.index[0] + pd.DateOffset(hours=1)]\n\n# Visualiser les données\nplt.figure(figsize=(15, 6))\nplt.plot(data_1hour.index, data_1hour['power'], label='Power')\nplt.xlabel('Time')\nplt.ylabel('Power')\nplt.title('Power Data for 1 Hour')\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-03T17:41:16.632764Z","iopub.execute_input":"2024-08-03T17:41:16.633034Z","iopub.status.idle":"2024-08-03T17:41:17.014436Z","shell.execute_reply.started":"2024-08-03T17:41:16.633010Z","shell.execute_reply":"2024-08-03T17:41:17.013475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filtrer les données pour une journée\ndata_1day = data.loc[data.index.date == data.index[0].date()]\n\n# Visualiser les données\nplt.figure(figsize=(15, 6))\nplt.plot(data_1day.index, data_1day['power'], label='Power')\nplt.xlabel('Time')\nplt.ylabel('Power')\nplt.title('Power Data for 1 Day')\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-03T17:41:17.015956Z","iopub.execute_input":"2024-08-03T17:41:17.016439Z","iopub.status.idle":"2024-08-03T17:41:19.308132Z","shell.execute_reply.started":"2024-08-03T17:41:17.016399Z","shell.execute_reply":"2024-08-03T17:41:19.307042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filtrer les données pour une semaine\ndata_1week = data.loc[data.index < data.index[0] + pd.DateOffset(weeks=1)]\n\n# Visualiser les données\nplt.figure(figsize=(15, 6))\nplt.plot(data_1week.index, data_1week['power'], label='Power')\nplt.xlabel('Time')\nplt.ylabel('Power')\nplt.title('Power Data for 1 Week')\nplt.legend()\nplt.grid(True)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-03T17:41:19.309591Z","iopub.execute_input":"2024-08-03T17:41:19.309945Z","iopub.status.idle":"2024-08-03T17:41:22.041224Z","shell.execute_reply.started":"2024-08-03T17:41:19.309917Z","shell.execute_reply":"2024-08-03T17:41:22.040173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filtrer les données pour un mois\ndata_1month = data.loc[data.index < data.index[0] + pd.DateOffset(months=1)]\n\n# Visualiser les données\nplt.figure(figsize=(15, 6))\nplt.plot(data_1month.index, data_1month['power'], label='Power')\nplt.xlabel('Time')\nplt.ylabel('Power')\nplt.title('Power Data for 1 Month')\nplt.legend()\nplt.grid(True)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-03T17:41:22.042394Z","iopub.execute_input":"2024-08-03T17:41:22.042694Z","iopub.status.idle":"2024-08-03T17:41:28.053023Z","shell.execute_reply.started":"2024-08-03T17:41:22.042669Z","shell.execute_reply":"2024-08-03T17:41:28.052047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the dataset (adjust the path to your dataset)\ndata = pd.read_csv('/kaggle/input/datazero2/output_modified.csv')\n\n# Check if 'time' column exists, convert it to datetime and set as index\nif 'time' in data.columns:\n    data['time'] = pd.to_datetime(data['time'])\n    data = data.set_index('time')\nelse:\n    print(\"The 'time' column does not exist\")\n\n# List of columns to plot\ncolumns_to_plot = ['power', 'power1', 'power2', 'power3']\n\n# Plot selected columns\nplt.figure(figsize=(15, 8))\nfor column in columns_to_plot:\n    if column in data.columns:\n        plt.plot(data.index, data[column], label=column)\nplt.xlabel('Time')\nplt.ylabel('Value')\nplt.title('Selected Power Data from DataFrame')\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-03T17:41:28.054380Z","iopub.execute_input":"2024-08-03T17:41:28.054668Z","iopub.status.idle":"2024-08-03T17:42:37.640556Z","shell.execute_reply.started":"2024-08-03T17:41:28.054642Z","shell.execute_reply":"2024-08-03T17:42:37.639618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Plot all columns except 'time'\nplt.figure(figsize=(15, 8))\nfor column in data.columns:\n    plt.plot(data.index, data[column], label=column)\nplt.xlabel('Time')\nplt.ylabel('Value')\nplt.title('Data from DataFrame')\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-03T17:42:37.641825Z","iopub.execute_input":"2024-08-03T17:42:37.642159Z","iopub.status.idle":"2024-08-03T17:44:14.300481Z","shell.execute_reply.started":"2024-08-03T17:42:37.642131Z","shell.execute_reply":"2024-08-03T17:44:14.299480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Analyse de serie temporelle*\"Analyse de la décomposition saisonnière de variable power\"**\n\nEst-ce un modéle additif ?**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\n# Charger le DataFrame à partir du fichier CSV\nfile_path = '/kaggle/input/datazero2/output_modified.csv'\ndf = pd.read_csv(file_path)\n\n# Convertir la colonne 'time' en type datetime et l'utiliser comme index\ndf['time'] = pd.to_datetime(df['time'])\n\n# Définir l'index sur la colonne 'time'\ndf.set_index('time', inplace=True)\n\n# Spécifier une période pour la décomposition saisonnière, par exemple 24 pour des données horaires\nperiod = 24\n\n# Effectuer la décomposition saisonnière avec un modèle additif\nTSA_additive = seasonal_decompose(df['power'], model='additive', period=period)\n\n# Afficher les composantes  de la décomposition saisonnière\nTSA_additive.plot()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-03T17:44:14.301780Z","iopub.execute_input":"2024-08-03T17:44:14.302138Z","iopub.status.idle":"2024-08-03T17:45:40.819351Z","shell.execute_reply.started":"2024-08-03T17:44:14.302108Z","shell.execute_reply":"2024-08-03T17:45:40.818480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Est-ce un modéle multiplicatif ?","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\n# Charger le DataFrame à partir du fichier CSV\nfile_path = '/kaggle/input/datazero2/output_modified.csv'\ndf = pd.read_csv(file_path)\n\n# Convertir la colonne 'time' en type datetime et l'utiliser comme index\ndf['time'] = pd.to_datetime(df['time'])\ndf.set_index('time', inplace=True)\n\n# Ajouter une petite constante aux valeurs pour les rendre positives\nconstant = 1e-8\ndf['power'] = df['power'] + constant\n\n# Spécifier une période pour la décomposition saisonnière, par exemple 24 pour des données horaires\nperiod = 24\n\n# Effectuer la décomposition saisonnière avec un modèle multiplicatif\nTSA_multiplicative = seasonal_decompose(df['power'], model='multiplicative', period=period)\n\n# Afficher les composantes de la décomposition saisonnière\nTSA_multiplicative.plot()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-03T17:45:40.820912Z","iopub.execute_input":"2024-08-03T17:45:40.821733Z","iopub.status.idle":"2024-08-03T17:47:06.953114Z","shell.execute_reply.started":"2024-08-03T17:45:40.821698Z","shell.execute_reply":"2024-08-03T17:47:06.952127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Fonctions pour l'analyse de séries temporelles et modélisation SARIMA**","metadata":{}},{"cell_type":"code","source":"# Function to plot ACF and PACF\ndef plot_acf_pacf(data, lags=7):\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n    plot_acf(data, lags=lags, ax=ax1)\n    plot_pacf(data, lags=lags, ax=ax2)\n    plt.show()\n\n# Seasonal decomposition\ndef seasonal_decompose_plot(data, period=24):\n    TSA = seasonal_decompose(data, period=period)\n    TSA.plot()\n\n# ADF Test\ndef adf_test(series):\n    result = adfuller(series)\n    print('ADF Statistic:', result[0])\n    print('p-value:', result[1])\n\n# Difference and ACF/PACF\ndef difference_acf_pacf(data, lags=36):\n    diff_power = data.diff().dropna()\n    plot_acf_pacf(diff_power, lags=lags)\n\n# Difference with seasonality and ACF/PACF\ndef seasonal_difference_acf_pacf(data, period=12, lags=36):\n    diff_power = data.diff().diff(period).dropna()\n    plot_acf_pacf(diff_power, lags=lags)\n\n# SARIMA model\ndef sarima_model(endog, order=(1,1,1), seasonal_order=(0,0,1,12)):\n    model = sm.tsa.SARIMAX(endog, order=order, seasonal_order=seasonal_order)\n    sarima = model.fit()\n    print(sarima.summary())","metadata":{"execution":{"iopub.status.busy":"2024-08-03T17:47:06.961484Z","iopub.execute_input":"2024-08-03T17:47:06.961774Z","iopub.status.idle":"2024-08-03T17:47:06.971675Z","shell.execute_reply.started":"2024-08-03T17:47:06.961751Z","shell.execute_reply":"2024-08-03T17:47:06.970790Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Test ADF**","metadata":{}},{"cell_type":"code","source":"#from statsmodels.tsa.stattools import adfuller\n#adf_test(data['power'])","metadata":{"execution":{"iopub.status.busy":"2024-08-03T17:47:06.972684Z","iopub.execute_input":"2024-08-03T17:47:06.972931Z","iopub.status.idle":"2024-08-03T17:47:06.986186Z","shell.execute_reply.started":"2024-08-03T17:47:06.972909Z","shell.execute_reply":"2024-08-03T17:47:06.985493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom statsmodels.tsa.stattools import adfuller\n\n# Exemple de réduction de l'échantillon\nsample_data = data['power'].sample(1000)\n\n# Exécution du test ADF sur l'échantillon\nadf_result = adfuller(sample_data)\n\nprint('ADF Statistic:', adf_result[0])\nprint('p-value:', adf_result[1])\nprint('Critical Values:')\nfor key, value in adf_result[4].items():\n    print(f'   {key}: {value}')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-03T17:47:06.987434Z","iopub.execute_input":"2024-08-03T17:47:06.987708Z","iopub.status.idle":"2024-08-03T17:47:07.068872Z","shell.execute_reply.started":"2024-08-03T17:47:06.987685Z","shell.execute_reply":"2024-08-03T17:47:07.067614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n****Nous rejetons l'hypothèse nulle du test ADF, ce qui signifie que la série temporelle data['power'] est stationnaire.","metadata":{}},{"cell_type":"markdown","source":"**Diffiérenciation + ACF/PACF\nutilisés principalement dans l'analyse des séries temporelles pour comprendre les relations et les dépendances temporelles dans les données.\nl'ACF et la PACF sont des outils essentiels pour analyser et modéliser les séries temporelles, en aidant à identifier les relations temporelles et à déterminer les paramètres appropriés pour les modèles de prévision.\n\n1)Sans difféerenciation**","metadata":{}},{"cell_type":"code","source":"difference_acf_pacf(df['power'])","metadata":{"execution":{"iopub.status.busy":"2024-08-03T17:47:07.070865Z","iopub.execute_input":"2024-08-03T17:47:07.071365Z","iopub.status.idle":"2024-08-03T17:50:39.717108Z","shell.execute_reply.started":"2024-08-03T17:47:07.071324Z","shell.execute_reply":"2024-08-03T17:50:39.716045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2)avec differenciation","metadata":{}},{"cell_type":"code","source":"seasonal_difference_acf_pacf(df['power'])","metadata":{"execution":{"iopub.status.busy":"2024-08-03T17:50:39.718652Z","iopub.execute_input":"2024-08-03T17:50:39.719406Z","iopub.status.idle":"2024-08-03T17:54:08.849701Z","shell.execute_reply.started":"2024-08-03T17:50:39.719358Z","shell.execute_reply":"2024-08-03T17:54:08.848705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"avec differencaition  d'ordre 12","metadata":{}},{"cell_type":"markdown","source":" **seasonal_decompose**\n","metadata":{}},{"cell_type":"code","source":" seasonal_decompose_plot(df['power'])","metadata":{"execution":{"iopub.status.busy":"2024-08-03T17:54:08.850833Z","iopub.execute_input":"2024-08-03T17:54:08.851156Z","iopub.status.idle":"2024-08-03T17:55:34.179703Z","shell.execute_reply.started":"2024-08-03T17:54:08.851130Z","shell.execute_reply":"2024-08-03T17:55:34.178754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**lissag exponentiell detection des anomalies a l'aide de fenetre glissent puis trace leur frequence **","metadata":{}},{"cell_type":"code","source":"##########version111111111\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Fonction pour appliquer le filtre EMA\ndef apply_ema_filter(data, alpha):\n    filtered_data = np.zeros_like(data)\n    filtered_data[0] = data[0]  # Initialiser le premier élément\n    for i in range(1, len(data)):\n        filtered_data[i] = alpha * data[i] + (1 - alpha) * filtered_data[i - 1]\n    return filtered_data\n\n# Fonction pour détecter les anomalies basées sur les moyennes glissantes\ndef check_samples_against_averages(samples, window_average, threshold=0.2):\n    anomalies = [\"normal\" if abs(sample - avg) <= threshold * avg else \"abnormal\"\n                 for sample, avg in zip(samples, window_average) if not np.isnan(avg)]\n    # Étendre la liste des anomalies pour correspondre à la longueur des échantillons, en supposant \"normal\" pour les valeurs initiales NaN\n    return [\"normal\"] * (len(samples) - len(anomalies)) + anomalies\n\n# Charger les données (remplacer le chemin si nécessaire)\ndata = pd.read_csv('/kaggle/input/datazero2/output_modified.csv')\n\n# Appliquer le lissage EMA aux colonnes spécifiques avec alpha = 0.8\nalpha = 0.8\ndata['smoothed_power'] = apply_ema_filter(data['power'].values, alpha)\ndata['smoothed_power1'] = apply_ema_filter(data['power1'].values, alpha)\ndata['smoothed_power2'] = apply_ema_filter(data['power2'].values, alpha)\ndata['smoothed_power3'] = apply_ema_filter(data['power3'].values, alpha)\n\n# Définir la taille de la fenêtre pour la détection d'anomalies\nwindow_size = 5\n\n# Appliquer la détection d'anomalies sur les données lissées en utilisant la fenêtre glissante\ndata['mwa_power'] = pd.Series(data['smoothed_power']).rolling(window=window_size).mean()\ndata['mwa_power1'] = pd.Series(data['smoothed_power1']).rolling(window=window_size).mean()\ndata['mwa_power2'] = pd.Series(data['smoothed_power2']).rolling(window=window_size).mean()\ndata['mwa_power3'] = pd.Series(data['smoothed_power3']).rolling(window=window_size).mean()\n\nthreshold = 0.10  # Ajuster le seuil pour la détection des anomalies\ndata['anomaly_power'] = check_samples_against_averages(data['smoothed_power'], data['mwa_power'], threshold)\ndata['anomaly_power1'] = check_samples_against_averages(data['smoothed_power1'], data['mwa_power1'], threshold)\ndata['anomaly_power2'] = check_samples_against_averages(data['smoothed_power2'], data['mwa_power2'], threshold)\ndata['anomaly_power3'] = check_samples_against_averages(data['smoothed_power3'], data['mwa_power3'], threshold)\n\n# Fonction pour tracer les données lissées\ndef plot_smoothed_data(data, column_name):\n    plt.figure(figsize=(12, 6))\n    plt.plot(data[column_name], label='Original', alpha=0.5)\n    plt.plot(data[f'smoothed_{column_name}'], label='Smoothed (EMA)', alpha=0.8)\n    plt.title(f'{column_name.capitalize()} Data with EMA Smoothing')\n    plt.xlabel('Index')\n    plt.ylabel(column_name.capitalize())\n    plt.legend()\n    plt.show()\n\n# Fonction pour tracer les anomalies\ndef plot_anomalies(data, column_name):\n    plt.figure(figsize=(12, 6))\n    plt.plot(data[f'smoothed_{column_name}'], label='Smoothed (EMA)', alpha=0.8)\n    plt.scatter(data.index[data[f'anomaly_{column_name}'] == 'abnormal'], \n                data[f'smoothed_{column_name}'][data[f'anomaly_{column_name}'] == 'abnormal'], \n                color='red', label='Anomalies', zorder=5)\n    plt.title(f'Anomalies in {column_name.capitalize()} Data')\n    plt.xlabel('Index')\n    plt.ylabel(column_name.capitalize())\n    plt.legend()\n    plt.show()\n\n# Tracer les graphiques pour chaque colonne\nfor column in ['power', 'power1', 'power2', 'power3']:\n    plot_smoothed_data(data, column)\n    plot_anomalies(data, column)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-03T17:55:34.181021Z","iopub.execute_input":"2024-08-03T17:55:34.181398Z","iopub.status.idle":"2024-08-03T17:56:01.489805Z","shell.execute_reply.started":"2024-08-03T17:55:34.181350Z","shell.execute_reply":"2024-08-03T17:56:01.488847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.fftpack import fft, ifft, fftfreq\n\n# Définir la fonction pour appliquer le filtre EMA\ndef apply_ema_filter(data, alpha):\n    filtered_data = np.zeros_like(data)\n    filtered_data[0] = data[0]  # Initialiser le premier élément\n    for i in range(1, len(data)):\n        filtered_data[i] = alpha * data[i] + (1 - alpha) * filtered_data[i - 1]\n    return filtered_data\n\n# Définir la fonction pour détecter les anomalies basées sur les moyennes glissantes\ndef check_samples_against_averages(samples, window_average, threshold=0.2):\n    anomalies = [\"normal\" if abs(sample - avg) <= threshold * avg else \"abnormal\"\n                 for sample, avg in zip(samples, window_average) if not np.isnan(avg)]\n    # Étendre la liste des anomalies pour correspondre à la longueur des échantillons, en supposant \"normal\" pour les valeurs initiales NaN\n    return [\"normal\"] * (len(samples) - len(anomalies)) + anomalies\n\n# Charger les données (remplacer le chemin si nécessaire)\ndata = pd.read_csv('/kaggle/input/datazero2/output_modified.csv')\n\n# Appliquer le lissage EMA aux colonnes spécifiques avec alpha = 0.8\nalpha = 0.8\ndata['smoothed_power'] = apply_ema_filter(data['power'].values, alpha)\ndata['smoothed_power1'] = apply_ema_filter(data['power1'].values, alpha)\ndata['smoothed_power2'] = apply_ema_filter(data['power2'].values, alpha)\ndata['smoothed_power3'] = apply_ema_filter(data['power3'].values, alpha)\n\n# Définir la taille de la fenêtre pour la détection d'anomalies\nwindow_size = 6\n\n# Appliquer la détection d'anomalies sur les données lissées en utilisant la fenêtre glissante\ndata['mwa_power'] = pd.Series(data['smoothed_power']).rolling(window=window_size).mean()\ndata['mwa_power1'] = pd.Series(data['smoothed_power1']).rolling(window=window_size).mean()\ndata['mwa_power2'] = pd.Series(data['smoothed_power2']).rolling(window=window_size).mean()\ndata['mwa_power3'] = pd.Series(data['smoothed_power3']).rolling(window=window_size).mean()\n\nthreshold = 0.10  # Ajuster le seuil pour la détection des anomalies\ndata['anomaly_power'] = check_samples_against_averages(data['smoothed_power'], data['mwa_power'], threshold)\ndata['anomaly_power1'] = check_samples_against_averages(data['smoothed_power1'], data['mwa_power1'], threshold)\ndata['anomaly_power2'] = check_samples_against_averages(data['smoothed_power2'], data['mwa_power2'], threshold)\ndata['anomaly_power3'] = check_samples_against_averages(data['smoothed_power3'], data['mwa_power3'], threshold)\n\n# Afficher les premières lignes pour chaque colonne avec les valeurs normales et anormales\nfor column in ['power', 'power1', 'power2', 'power3']:\n    print(f\"First 30 entries for {column} with anomalies:\")\n    print(data[[column, f'smoothed_{column}', f'anomaly_{column}']].head(30))\n    print(\"\\n\")\n\n# Fonction pour tracer les données lissées\ndef plot_smoothed_data(data, column_name):\n    plt.figure(figsize=(12, 6))\n    plt.plot(data[column_name], label='Original', alpha=0.5)\n    plt.plot(data[f'smoothed_{column_name}'], label='Smoothed (EMA)', alpha=0.8)\n    plt.title(f'{column_name.capitalize()} Data with EMA Smoothing')\n    plt.xlabel('Index')\n    plt.ylabel(column_name.capitalize())\n    plt.legend()\n    plt.show()\n\n# Fonction pour tracer les anomalies\ndef plot_anomalies(data, column_name):\n    plt.figure(figsize=(12, 6))\n    plt.plot(data[f'smoothed_{column_name}'], label='Smoothed (EMA)', alpha=0.8)\n    plt.scatter(data.index[data[f'anomaly_{column_name}'] == 'abnormal'], \n                data[f'smoothed_{column_name}'][data[f'anomaly_{column_name}'] == 'abnormal'], \n                color='red', label='Anomalies', zorder=5)\n    plt.title(f'Anomalies in {column_name.capitalize()} Data')\n    plt.xlabel('Index')\n    plt.ylabel(column_name.capitalize())\n    plt.legend()\n    plt.show()\n\n# Fonction pour tracer les anomalies en fréquence\ndef plot_anomalies_frequency(data, column_name):\n    # Extraire les indices et valeurs anormales\n    abnormal_indices = data.index[data[f'anomaly_{column_name}'] == 'abnormal']\n    abnormal_values = data[f'smoothed_{column_name}'][data[f'anomaly_{column_name}'] == 'abnormal'].values\n    \n    # Tracer les anomalies\n    plt.figure(figsize=(12, 6))\n    plt.plot(abnormal_indices, abnormal_values, 'ro-', label='Abnormal', zorder=5)\n    plt.title(f'Anomalous Points in {column_name.capitalize()} Data')\n    plt.xlabel('Index')\n    plt.ylabel(column_name.capitalize())\n    plt.legend()\n    plt.show()\n    \n    # Transformation de Fourier pour analyser les fréquences des anomalies\n    n = len(abnormal_values)\n    yf = fft(abnormal_values)\n    xf = fftfreq(n, 1)  # Période d'échantillonnage de 1 unité\n\n    # Filtrer les fréquences basses et hautes (ex. : couper en dessous de 0.1 et au-dessus de 0.9)\n    yf_filtered = np.copy(yf)\n    low_cutoff = 0.1\n    high_cutoff = 0.9\n    yf_filtered[(xf < low_cutoff) | (xf > high_cutoff)] = 0\n\n    # Transformer de nouveau dans le domaine temporel\n    filtered_anomalies = ifft(yf_filtered).real\n\n    # Créer un graphique pour les anomalies filtrées\n    plt.figure(figsize=(12, 6))\n    plt.plot(abnormal_indices, filtered_anomalies, 'go-', label='Filtered Anomalies', zorder=5)\n    plt.title(f'Filtered Anomalous Points in {column_name.capitalize()} Data (Frequency Domain)')\n    plt.xlabel('Index')\n    plt.ylabel('Filtered ' + column_name.capitalize())\n    plt.legend()\n    plt.show()\n\n# Tracer les graphiques pour chaque colonne\nfor column in ['power', 'power1', 'power2', 'power3']:\n    plot_smoothed_data(data, column)\n    plot_anomalies(data, column)\n    plot_anomalies_frequency(data, column)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-03T17:56:01.491117Z","iopub.execute_input":"2024-08-03T17:56:01.491437Z","iopub.status.idle":"2024-08-03T17:56:33.492237Z","shell.execute_reply.started":"2024-08-03T17:56:01.491404Z","shell.execute_reply":"2024-08-03T17:56:33.491273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"####version2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.fftpack import fft, ifft, fftfreq\n\n# Définir la fonction pour appliquer le filtre EMA\ndef apply_ema_filter(data, alpha):\n    filtered_data = np.zeros_like(data)\n    filtered_data[0] = data[0]  # Initialiser le premier élément\n    for i in range(1, len(data)):\n        filtered_data[i] = alpha * data[i] + (1 - alpha) * filtered_data[i - 1]\n    return filtered_data\n\n# Définir la fonction pour détecter les anomalies basées sur les moyennes glissantes\ndef check_samples_against_averages(samples, window_average, threshold=0.2):\n    anomalies = [\"normal\" if abs(sample - avg) <= threshold * avg else \"abnormal\"\n                 for sample, avg in zip(samples, window_average) if not np.isnan(avg)]\n    # Étendre la liste des anomalies pour correspondre à la longueur des échantillons, en supposant \"normal\" pour les valeurs initiales NaN\n    return [\"normal\"] * (len(samples) - len(anomalies)) + anomalies\n\n# Charger les données (remplacer le chemin si nécessaire)\ndata = pd.read_csv('/kaggle/input/datazero2/output_modified.csv')\n\n# Appliquer le lissage EMA aux colonnes spécifiques avec alpha = 0.8\nalpha = 0.8\ndata['smoothed_power'] = apply_ema_filter(data['power'].values, alpha)\ndata['smoothed_power1'] = apply_ema_filter(data['power1'].values, alpha)\ndata['smoothed_power2'] = apply_ema_filter(data['power2'].values, alpha)\ndata['smoothed_power3'] = apply_ema_filter(data['power3'].values, alpha)\n\n# Définir la taille de la fenêtre pour la détection d'anomalies\nwindow_size = 5\n\n# Appliquer la détection d'anomalies sur les données lissées en utilisant la fenêtre glissante\ndata['mwa_power'] = pd.Series(data['smoothed_power']).rolling(window=window_size).mean()\ndata['mwa_power1'] = pd.Series(data['smoothed_power1']).rolling(window=window_size).mean()\ndata['mwa_power2'] = pd.Series(data['smoothed_power2']).rolling(window=window_size).mean()\ndata['mwa_power3'] = pd.Series(data['smoothed_power3']).rolling(window=window_size).mean()\n\nthreshold = 0.10  # Ajuster le seuil pour la détection des anomalies\ndata['anomaly_power'] = check_samples_against_averages(data['smoothed_power'], data['mwa_power'], threshold)\ndata['anomaly_power1'] = check_samples_against_averages(data['smoothed_power1'], data['mwa_power1'], threshold)\ndata['anomaly_power2'] = check_samples_against_averages(data['smoothed_power2'], data['mwa_power2'], threshold)\ndata['anomaly_power3'] = check_samples_against_averages(data['smoothed_power3'], data['mwa_power3'], threshold)\n\n# Fonction pour tracer les données lissées\ndef plot_smoothed_data(data, column_name):\n    plt.figure(figsize=(12, 6))\n    plt.plot(data[column_name], label='Original', alpha=0.5)\n    plt.plot(data[f'smoothed_{column_name}'], label='Smoothed (EMA)', alpha=0.8)\n    plt.title(f'{column_name.capitalize()} Data with EMA Smoothing')\n    plt.xlabel('Index')\n    plt.ylabel(column_name.capitalize())\n    plt.legend()\n    plt.show()\n\n# Fonction pour tracer les anomalies\ndef plot_anomalies(data, column_name):\n    plt.figure(figsize=(12, 6))\n    plt.plot(data[f'smoothed_{column_name}'], label='Smoothed (EMA)', alpha=0.8)\n    plt.scatter(data.index[data[f'anomaly_{column_name}'] == 'abnormal'], \n                data[f'smoothed_{column_name}'][data[f'anomaly_{column_name}'] == 'abnormal'], \n                color='red', label='Anomalies', zorder=5)\n    plt.title(f'Anomalies in {column_name.capitalize()} Data')\n    plt.xlabel('Index')\n    plt.ylabel(column_name.capitalize())\n    plt.legend()\n    plt.show()\n\n# Fonction pour tracer les anomalies en fréquence\ndef plot_anomalies_frequency(data, column_name):\n    # Extraire les indices et valeurs anormales\n    abnormal_indices = data.index[data[f'anomaly_{column_name}'] == 'abnormal']\n    abnormal_values = data[f'smoothed_{column_name}'][data[f'anomaly_{column_name}'] == 'abnormal'].values\n    \n    # Tracer les anomalies\n    plt.figure(figsize=(12, 6))\n    plt.plot(abnormal_indices, abnormal_values, 'ro-', label='Abnormal', zorder=5)\n    plt.title(f'Anomalous Points in {column_name.capitalize()} Data')\n    plt.xlabel('Index')\n    plt.ylabel(column_name.capitalize())\n    plt.legend()\n    plt.show()\n    \n    # Transformation de Fourier pour analyser les fréquences des anomalies\n    n = len(abnormal_values)\n    yf = fft(abnormal_values)\n    xf = fftfreq(n, 1)  # Période d'échantillonnage de 1 unité\n\n    # Filtrer les fréquences basses et hautes (ex. : couper en dessous de 0.1 et au-dessus de 0.9)\n    yf_filtered = np.copy(yf)\n    low_cutoff = 0.1\n    high_cutoff = 0.9\n    yf_filtered[(xf < low_cutoff) | (xf > high_cutoff)] = 0\n\n    # Transformer de nouveau dans le domaine temporel\n    filtered_anomalies = ifft(yf_filtered).real\n\n    # Créer un graphique pour les anomalies filtrées\n    plt.figure(figsize=(12, 6))\n    plt.plot(abnormal_indices, filtered_anomalies, 'go-', label='Filtered Anomalies', zorder=5)\n    plt.title(f'Filtered Anomalous Points in {column_name.capitalize()} Data (Frequency Domain)')\n    plt.xlabel('Index')\n    plt.ylabel('Filtered ' + column_name.capitalize())\n    plt.legend()\n    plt.show()\n\n# Tracer les graphiques pour chaque colonne\nfor column in ['power', 'power1', 'power2', 'power3']:\n    plot_smoothed_data(data, column)\n    plot_anomalies(data, column)\n    plot_anomalies_frequency(data, column)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-03T17:56:33.493983Z","iopub.execute_input":"2024-08-03T17:56:33.494360Z","iopub.status.idle":"2024-08-03T17:57:05.537447Z","shell.execute_reply.started":"2024-08-03T17:56:33.494328Z","shell.execute_reply":"2024-08-03T17:57:05.536422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**versionn3**","metadata":{}},{"cell_type":"code","source":"# Fonction pour appliquer le filtre EMA\ndef apply_ema_filter(data, alpha):\n    filtered_data = np.zeros_like(data)\n    filtered_data[0] = data[0]  # Initialiser le premier élément\n    for i in range(1, len(data)):\n        filtered_data[i] = alpha * data[i] + (1 - alpha) * filtered_data[i - 1]\n    return filtered_data\n\n# Fonction pour appliquer la moyenne glissante\ndef moving_window_average(data, window_size):\n    return data.rolling(window=window_size).mean()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-03T17:57:05.538726Z","iopub.execute_input":"2024-08-03T17:57:05.539020Z","iopub.status.idle":"2024-08-03T17:57:05.545638Z","shell.execute_reply.started":"2024-08-03T17:57:05.538994Z","shell.execute_reply":"2024-08-03T17:57:05.544578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Appliquer le lissage EMA aux colonnes spécifiques avec alpha = 0.8\nalpha = 0.8\ndata['smoothed_power'] = apply_ema_filter(data['power'].values, alpha)\ndata['smoothed_power1'] = apply_ema_filter(data['power1'].values, alpha)\ndata['smoothed_power2'] = apply_ema_filter(data['power2'].values, alpha)\ndata['smoothed_power3'] = apply_ema_filter(data['power3'].values, alpha)\n\n# Afficher les premières lignes du DataFrame mis à jour pour vérifier les valeurs lissées\nprint(data[['power', 'smoothed_power', 'power1', 'smoothed_power1', 'power2', 'smoothed_power2', 'power3', 'smoothed_power3']].head())\n","metadata":{"execution":{"iopub.status.busy":"2024-08-03T17:57:05.546752Z","iopub.execute_input":"2024-08-03T17:57:05.547022Z","iopub.status.idle":"2024-08-03T17:57:09.677140Z","shell.execute_reply.started":"2024-08-03T17:57:05.546999Z","shell.execute_reply":"2024-08-03T17:57:09.676073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Définir le chemin pour le fichier CSV des données lissées\nsmoothed_data_path = './smoothed_datas.csv'\n\n# Sauvegarder les données lissées dans un fichier CSV\nsmoothed_columns = ['smoothed_power', 'smoothed_power1', 'smoothed_power2', 'smoothed_power3']\ndata[smoothed_columns].to_csv(smoothed_data_path, index=False)\n\nprint(f'Données lissées sauvegardées dans {smoothed_data_path}')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-03T17:57:09.678581Z","iopub.execute_input":"2024-08-03T17:57:09.679021Z","iopub.status.idle":"2024-08-03T17:57:17.006284Z","shell.execute_reply.started":"2024-08-03T17:57:09.678982Z","shell.execute_reply":"2024-08-03T17:57:17.005311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Appliquer la moyenne glissante avec une taille de fenêtre de 5\nwindow_size = 5\ndata['mwa_power'] = moving_window_average(data['smoothed_power'], window_size)\ndata['mwa_power1'] = moving_window_average(data['smoothed_power1'], window_size)\ndata['mwa_power2'] = moving_window_average(data['smoothed_power2'], window_size)\ndata['mwa_power3'] = moving_window_average(data['smoothed_power3'], window_size)\n\n# Afficher les premières lignes du DataFrame mis à jour pour vérifier les moyennes glissantes\ndata[['smoothed_power', 'mwa_power', 'smoothed_power1', 'mwa_power1', 'smoothed_power2', 'mwa_power2', 'smoothed_power3', 'mwa_power3']].head(10)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-03T17:57:17.007716Z","iopub.execute_input":"2024-08-03T17:57:17.008346Z","iopub.status.idle":"2024-08-03T17:57:17.114766Z","shell.execute_reply.started":"2024-08-03T17:57:17.008306Z","shell.execute_reply":"2024-08-03T17:57:17.113844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Définir le chemin pour le fichier CSV des données finales avec les moyennes glissantes\nfinal_data_path = './final_smoothed_data.csv'\n\n# Colonnes à sauvegarder\nfinal_columns = ['mwa_power', 'mwa_power1', 'mwa_power2', 'mwa_power3']\n\n# Sauvegarder les données finales avec les moyennes glissantes dans un fichier CSV\ndata[final_columns].to_csv(final_data_path, index=False)\n\nprint(f'Données finales sauvegardées dans {final_data_path}')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-03T17:57:17.116020Z","iopub.execute_input":"2024-08-03T17:57:17.116434Z","iopub.status.idle":"2024-08-03T17:57:24.503315Z","shell.execute_reply.started":"2024-08-03T17:57:17.116372Z","shell.execute_reply":"2024-08-03T17:57:24.502119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fonction pour détecter les anomalies basées sur les moyennes glissantes\ndef check_samples_against_averages(samples, window_average, threshold=0.20):\n    anomalies = [\"normal\" if abs(sample - avg) > threshold * avg else \"abnormal\"\n                 for sample, avg in zip(samples, window_average) if not np.isnan(avg)]\n    # Étendre la liste des anomalies pour correspondre à la longueur des échantillons, en supposant \"normal\" pour les valeurs initiales NaN\n    return [\"normal\"] * (len(samples) - len(anomalies)) + anomalies\n\n# Appliquer la détection d'anomalies\nthreshold = 0.10  # Ajuster le seuil pour la détection des anomalies\ndata['anomaly_power'] = check_samples_against_averages(data['smoothed_power'], data['mwa_power'], threshold)\ndata['anomaly_power1'] = check_samples_against_averages(data['smoothed_power1'], data['mwa_power1'], threshold)\ndata['anomaly_power2'] = check_samples_against_averages(data['smoothed_power2'], data['mwa_power2'], threshold)\ndata['anomaly_power3'] = check_samples_against_averages(data['smoothed_power3'], data['mwa_power3'], threshold)\n\n# Vérifier les résultats avec les anomalies\ndata[['power', 'smoothed_power', 'mwa_power', 'anomaly_power']].head(10)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-03T17:57:24.504827Z","iopub.execute_input":"2024-08-03T17:57:24.505234Z","iopub.status.idle":"2024-08-03T17:57:32.141130Z","shell.execute_reply.started":"2024-08-03T17:57:24.505199Z","shell.execute_reply":"2024-08-03T17:57:32.140088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fonction pour tracer les données\ndef plot_data(data, column_name):\n    plt.figure(figsize=(12, 6))\n    plt.plot(data[column_name], label='Original', alpha=0.5)\n    plt.plot(data[f'smoothed_{column_name}'], label='Smoothed', alpha=0.8)\n    plt.plot(data[f'mwa_{column_name}'], label='Moving Window Average', linestyle='--')\n    plt.title(f'{column_name.capitalize()} Data')\n    plt.xlabel('Index')\n    plt.ylabel(column_name.capitalize())\n    plt.legend()\n    plt.show()\n\n# Tracer chaque colonne\nfor column in ['power', 'power1', 'power2', 'power3']:\n    plot_data(data, column)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-03T17:57:32.142387Z","iopub.execute_input":"2024-08-03T17:57:32.142706Z","iopub.status.idle":"2024-08-03T17:58:13.119874Z","shell.execute_reply.started":"2024-08-03T17:57:32.142679Z","shell.execute_reply":"2024-08-03T17:58:13.118951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Définir la fonction pour appliquer le filtre EMA\ndef apply_ema_filter(data, alpha):\n    filtered_data = np.zeros_like(data)\n    filtered_data[0] = data[0]  # Initialiser le premier élément\n    for i in range(1, len(data)):\n        filtered_data[i] = alpha * data[i] + (1 - alpha) * filtered_data[i - 1]\n    return filtered_data\n\n# Définir la fonction pour appliquer la moyenne glissante\ndef moving_window_average(data, window_size):\n    return data.rolling(window=window_size).mean()\n\n# Extraire la liste des valeurs de puissance\npower_list = list(data[\"power\"])\nx_power = np.arange(len(power_list))\n\n# Appliquer le filtre EMA\nfiltered_power_list = apply_ema_filter(power_list, 0.8)\n\n# Calculer la moyenne glissante avec une fenêtre de 5\nmwa_power = moving_window_average(pd.Series(filtered_power_list), window_size=5).values\n\n# Définir la fonction pour détecter les anomalies basées sur les moyennes glissantes\ndef check_samples_against_averages(samples, window_average, threshold=0.2):\n    anomalies = [\"normal\" if abs(sample - avg) <= threshold * avg else \"abnormal\"\n                 for sample, avg in zip(samples, window_average) if not np.isnan(avg)]\n    # Étendre la liste des anomalies pour correspondre à la longueur des échantillons, en supposant \"normal\" pour les valeurs initiales NaN\n    return [\"normal\"] * (len(samples) - len(anomalies)) + anomalies\n\n# Détecter les anomalies en utilisant les moyennes glissantes calculées\nthreshold = 0.10  # Ajuster le seuil pour la détection des anomalies\nresults = check_samples_against_averages(filtered_power_list, mwa_power, threshold)\n\n# Tracer les résultats\nplt.figure(figsize=(12, 6))\nplt.plot(x_power, filtered_power_list, label=\"Filtered Power\", color='blue')\n\n# Mettre en évidence les échantillons anormaux\nabnormal_indices = [i for i, result in enumerate(results) if result == \"abnormal\"]\nabnormal_values = [filtered_power_list[i] for i in abnormal_indices]\nplt.scatter(abnormal_indices, abnormal_values, color='red', label='Abnormal', zorder=5)\n\n# Ajouter les titres et légendes\nplt.title(\"Filtered Power with Abnormal Points Highlighted\")\nplt.xlabel(\"Sample Index\")\nplt.ylabel(\"Power\")\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-03T17:58:13.121474Z","iopub.execute_input":"2024-08-03T17:58:13.121741Z","iopub.status.idle":"2024-08-03T17:58:17.916234Z","shell.execute_reply.started":"2024-08-03T17:58:13.121717Z","shell.execute_reply":"2024-08-03T17:58:17.915295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.fftpack import fft, ifft, fftfreq\n\n\n\n# Transformation Fourier pour analyser les fréquences des anomalies\ndef plot_filtered_anomalous_points(abnormal_indices, abnormal_values):\n    n = len(abnormal_values)\n    yf = fft(abnormal_values)\n    xf = fftfreq(n, 1)  # Période d'échantillonnage de 1 unité\n\n    # Filtrer les fréquences basses et hautes (ex. : couper en dessous de 0.1 et au-dessus de 0.9)\n    yf_filtered = np.copy(yf)\n    low_cutoff = 0.1\n    high_cutoff = 0.9\n    yf_filtered[(xf < low_cutoff) | (xf > high_cutoff)] = 0\n\n    # Transformer de nouveau dans le domaine temporel\n    filtered_anomalies = ifft(yf_filtered).real\n\n    # Créer un graphique pour les anomalies filtrées\n    plt.figure(figsize=(12, 6))\n    plt.plot(abnormal_indices, filtered_anomalies, 'go-', label='Filtered Anomalies', zorder=5)\n\n    # Ajouter les titres et légendes pour le graphique des anomalies filtrées\n    plt.title(\"Filtered Anomalous Points (Frequency Domain)\")\n    plt.xlabel(\"Sample Index\")\n    plt.ylabel(\"Filtered Power\")\n    plt.legend()\n    plt.show()\n\n# Appeler la fonction pour tracer les points anormaux filtrés\nabnormal_indices = [i for i, result in enumerate(results) if result == \"abnormal\"]\nabnormal_values = [filtered_power_list[i] for i in abnormal_indices]\nplot_filtered_anomalous_points(abnormal_indices, abnormal_values)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-03T17:58:17.917476Z","iopub.execute_input":"2024-08-03T17:58:17.917746Z","iopub.status.idle":"2024-08-03T17:58:18.460129Z","shell.execute_reply.started":"2024-08-03T17:58:17.917723Z","shell.execute_reply":"2024-08-03T17:58:18.459014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**test de code**","metadata":{}},{"cell_type":"code","source":"############afffiche en ------------\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nimport time\nimport matplotlib.pyplot as plt\n\n# Charger les données depuis le fichier CSV\ndata_path = '/kaggle/input/datazero2/output_modified.csv'\ndf = pd.read_csv(data_path)\n\n# Assurer que la colonne 'time' est au format datetime\ndf['time'] = pd.to_datetime(df['time'])\n\n# Fonction pour préparer les séquences temporelles\ndef prepare_sequences(data, window_size):\n    X, y = [], []\n    for i in range(len(data) - window_size):\n        X.append(data[i:i + window_size])\n        y.append(data[i + window_size])\n    return np.array(X), np.array(y)\n\n# Fonction pour détecter les anomalies (pics et chutes)\ndef detect_anomalies(data, pos_threshold=2, neg_threshold=2):\n    mean = np.mean(data)\n    std = np.std(data)\n    pos_anomalies = np.where(data - mean > pos_threshold * std)[0]\n    neg_anomalies = np.where(data - mean < -neg_threshold * std)[0]\n    abnormal_indices = np.concatenate([pos_anomalies, neg_anomalies])\n    abnormal_values = data[abnormal_indices]\n    return abnormal_values, abnormal_indices\n\n# Fonction pour entraîner le modèle et tracer les résultats\ndef process_power_column(df, column_name, window_size=10):\n    # Sélectionner la variable de temps et la série temporelle à prévoir\n    time_series = df['time']\n    target_variable = df[column_name]\n\n    # Normaliser les données\n    target_variable = (target_variable - target_variable.mean()) / target_variable.std()\n\n    # Préparer les données en séquences temporelles\n    X, y = prepare_sequences(target_variable.values, window_size)\n\n    # Diviser les données en ensembles d'entraînement et de test\n    split = int(0.8 * len(X))\n    X_train, X_test = X[:split], X[split:]\n    y_train, y_test = y[:split], y[split:]\n\n    # Reshape les données pour qu'elles soient compatibles avec le modèle GRU\n    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n\n    # Enregistrer l'heure de début\n    start_time = time.time()\n\n    # Définir le modèle GRU\n    model = tf.keras.Sequential([\n        tf.keras.layers.GRU(64, input_shape=(window_size, 1)),\n        tf.keras.layers.Dense(1)\n    ])\n\n    # Compiler le modèle\n    model.compile(optimizer='adam', loss='mean_squared_error')\n\n    # Définir l'early stopping\n    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n\n    # Ajuster le modèle aux données d'entraînement\n    history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n\n    # Prévoir les valeurs futures\n    forecast = model.predict(X_test)\n\n    # Enregistrer l'heure de fin\n    end_time = time.time()\n\n    # Calculer le temps d'exécution en secondes\n    execution_time = end_time - start_time\n\n    # Calcul des métriques\n    mae = mean_absolute_error(y_test, forecast)\n    rmse = np.sqrt(mean_squared_error(y_test, forecast))\n\n    # Afficher les métriques\n    print(f\"Metrics for {column_name}\")\n    print(\"MAE:\", mae)\n    print(\"RMSE:\", rmse)\n    print(\"Temps d'exécution (TT) en secondes:\", execution_time)\n\n    # Détecter les anomalies dans l'ensemble de test\n    abnormal_values, abnormal_indices = detect_anomalies(y_test)\n\n    # Convertir les indices des anomalies en indices temporels\n    abnormal_time_indices = df['time'][split + window_size + abnormal_indices]\n\n    # Tracer les vraies valeurs par rapport aux prédictions\n    plt.figure(figsize=(10, 6))\n    plt.plot(df['time'][split + window_size:], y_test, label='Vraies valeurs')\n    plt.plot(df['time'][split + window_size:], forecast, label='Prédictions')\n\n    # Ajouter des points rouges pour les anomalies\n    plt.scatter(abnormal_time_indices, abnormal_values, color='red', label='Anomalies')\n\n    plt.xlabel('Temps')\n    plt.ylabel(f'{column_name} (Normalisé)')\n    plt.title(f'Vraies valeurs vs Prédictions pour {column_name}')\n    plt.legend()\n    plt.show()\n\n# Exemple d'utilisation\nfor column in ['power', 'power1', 'power2', 'power3']:\n    process_power_column(df, column)\n##########################################333nooooooooooooooooooooooooooooooooooooooooooooooooo","metadata":{"execution":{"iopub.status.busy":"2024-08-03T17:58:18.461755Z","iopub.execute_input":"2024-08-03T17:58:18.462050Z","iopub.status.idle":"2024-08-03T18:41:35.625258Z","shell.execute_reply.started":"2024-08-03T17:58:18.462024Z","shell.execute_reply":"2024-08-03T18:41:35.623371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"code bera7            ","metadata":{}},{"cell_type":"code","source":"###affiche en +++++\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\nimport time\nimport matplotlib.pyplot as plt\n\n# Charger les données depuis le fichier CSV\ndata_path = '/kaggle/input/datazero2/output_modified.csv'\ndf = pd.read_csv(data_path)\n\n# Assurer que la colonne 'time' est au format datetime\ndf['time'] = pd.to_datetime(df['time'])\n\n# Fonction pour préparer les séquences temporelles\ndef prepare_sequences(data, window_size):\n    X, y = [], []\n    for i in range(len(data) - window_size):\n        X.append(data[i:i + window_size])\n        y.append(data[i + window_size])\n    return np.array(X), np.array(y)\n\n# Fonction pour détecter les anomalies (pics et chutes)\ndef detect_anomalies(data, threshold=2):\n    mean = np.mean(data)\n    std = np.std(data)\n    abnormal_indices = np.where(np.abs(data - mean) > threshold * std)[0]\n    abnormal_values = data[abnormal_indices]\n    return abnormal_values, abnormal_indices\n\n# Fonction pour entraîner le modèle et tracer les résultats\ndef process_power_column(df, column_name, window_size=10):\n    # Sélectionner la variable de temps et la série temporelle à prévoir\n    time_series = df['time']\n    target_variable = df[column_name].values.reshape(-1, 1)\n\n    # Normaliser les données pour qu'elles soient entre 0 et 1\n    scaler = MinMaxScaler()\n    target_variable = scaler.fit_transform(target_variable)\n\n    # Préparer les données en séquences temporelles\n    X, y = prepare_sequences(target_variable, window_size)\n\n    # Diviser les données en ensembles d'entraînement et de test\n    split = int(0.8 * len(X))\n    X_train, X_test = X[:split], X[split:]\n    y_train, y_test = y[:split], y[split:]\n\n    # Reshape les données pour qu'elles soient compatibles avec le modèle GRU\n    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n\n    # Enregistrer l'heure de début\n    start_time = time.time()\n\n    # Définir le modèle GRU\n    model = tf.keras.Sequential([\n        tf.keras.layers.GRU(128, return_sequences=True, input_shape=(window_size, 1)),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.GRU(64, return_sequences=True),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.GRU(32),\n        tf.keras.layers.Dense(32, activation='relu'),\n        tf.keras.layers.Dense(1)\n    ])\n\n    # Compiler le modèle\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error')\n\n    # Définir l'early stopping et le réduction du taux d'apprentissage\n    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-6)\n\n    # Ajuster le modèle aux données d'entraînement\n    history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, callbacks=[early_stopping, reduce_lr])\n\n    # Prévoir les valeurs futures\n    forecast = model.predict(X_test)\n\n    # Inverser la normalisation pour les prédictions et les vraies valeurs\n    forecast = scaler.inverse_transform(forecast)\n    y_test = scaler.inverse_transform(y_test)\n\n    # Enregistrer l'heure de fin\n    end_time = time.time()\n\n    # Calculer le temps d'exécution en secondes\n    execution_time = end_time - start_time\n\n    # Calcul des métriques\n    mae = mean_absolute_error(y_test, forecast)\n    rmse = np.sqrt(mean_squared_error(y_test, forecast))\n\n    # Afficher les métriques\n    print(f\"Metrics for {column_name}\")\n    print(\"MAE:\", mae)\n    print(\"RMSE:\", rmse)\n    print(\"Temps d'exécution (TT) en secondes:\", execution_time)\n\n    # Détecter les anomalies dans l'ensemble de test\n    abnormal_values, abnormal_indices = detect_anomalies(y_test.flatten())\n\n    # Convertir les indices des anomalies en indices temporels\n    abnormal_time_indices = df['time'][split + window_size + abnormal_indices]\n\n    # Tracer les vraies valeurs par rapport aux prédictions\n    plt.figure(figsize=(10, 6))\n    plt.plot(df['time'][split + window_size:], y_test, label='Vraies valeurs')\n    plt.plot(df['time'][split + window_size:], forecast, label='Prédictions')\n\n    # Ajouter des points rouges pour les anomalies\n    plt.scatter(abnormal_time_indices, abnormal_values, color='red', label='Anomalies')\n\n    plt.xlabel('Temps')\n    plt.ylabel(f'{column_name}')\n    plt.title(f'Vraies valeurs vs Prédictions pour {column_name}')\n    plt.legend()\n    plt.show()\n\n    # Tracer les graphes de loss et val_loss\n    plt.figure(figsize=(10, 6))\n    plt.plot(history.history['loss'], label='loss')\n    plt.plot(history.history['val_loss'], label='val_loss')\n    plt.xlabel('Époque')\n    plt.ylabel('Perte')\n    plt.title('Perte et Perte de validation au cours de l\\'entraînement')\n    plt.legend()\n    plt.show()\n\n# Exemple d'utilisation\nfor column in ['power', 'power1', 'power2', 'power3']:\n    process_power_column(df, column)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-03T18:41:45.585334Z","iopub.execute_input":"2024-08-03T18:41:45.585690Z","iopub.status.idle":"2024-08-03T19:40:51.861023Z","shell.execute_reply.started":"2024-08-03T18:41:45.585654Z","shell.execute_reply":"2024-08-03T19:40:51.859765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nimport os\n\n# Fonction pour compresser les répertoires de modèles sauvegardés\ndef compress_model(model_name):\n    shutil.make_archive(f'{model_name}_model', 'zip', f'{model_name}_saved_model')\n    print(f'Compressed {model_name}_saved_model to {model_name}_model.zip')\n\n# Fonction pour copier les fichiers si nécessaire\ndef copy_file(src, dest):\n    if os.path.exists(src):\n        if os.path.abspath(src) != os.path.abspath(dest):\n            shutil.copy(src, dest)\n            print(f'Copied {src} to {dest}')\n        else:\n            print(f'Skipped copying {src} to {dest} (same file)')\n    else:\n        print(f'Source file {src} does not exist')\n\n# Compresser les fichiers de modèles sauvegardés\ncompress_model('power')\ncompress_model('power1')\ncompress_model('power2')\ncompress_model('power3')\n\n# Ajouter les fichiers .h5 et les archives zip en tant que fichiers de sortie\ncopy_file('power_model.h5', '/kaggle/working/power_model.h5')\ncopy_file('power1_model.h5', '/kaggle/working/power1_model.h5')\ncopy_file('power2_model.h5', '/kaggle/working/power2_model.h5')\ncopy_file('power3_model.h5', '/kaggle/working/power3_model.h5')\n\ncopy_file('power_model.zip', '/kaggle/working/power_model.zip')\ncopy_file('power1_model.zip', '/kaggle/working/power1_model.zip')\ncopy_file('power2_model.zip', '/kaggle/working/power2_model.zip')\ncopy_file('power3_model.zip', '/kaggle/working/power3_model.zip')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-03T18:41:35.631850Z","iopub.status.idle":"2024-08-03T18:41:35.632254Z","shell.execute_reply.started":"2024-08-03T18:41:35.632053Z","shell.execute_reply":"2024-08-03T18:41:35.632086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nimport os\n\n# Fonction pour compresser les répertoires de modèles sauvegardés\ndef compress_model(model_name):\n    shutil.make_archive(f'{model_name}_model', 'zip', f'{model_name}_saved_model')\n\n# Fonction pour copier les fichiers si nécessaire\ndef copy_file(src, dest):\n    # Vérifier si le fichier source et le fichier de destination sont différents\n    if os.path.abspath(src) != os.path.abspath(dest):\n        shutil.copy(src, dest)\n\n# Compresser les fichiers de modèles sauvegardés\ncompress_model('power')\ncompress_model('power1')\ncompress_model('power2')\ncompress_model('power3')\n\n# Ajouter les fichiers .h5 et les archives zip en tant que fichiers de sortie\ncopy_file('power_model.h5', '/kaggle/working/power_model.h5')\ncopy_file('power1_model.h5', '/kaggle/working/power1_model.h5')\ncopy_file('power2_model.h5', '/kaggle/working/power2_model.h5')\ncopy_file('power3_model.h5', '/kaggle/working/power3_model.h5')\n\ncopy_file('power_model.zip', '/kaggle/working/power_model.zip')\ncopy_file('power1_model.zip', '/kaggle/working/power1_model.zip')\ncopy_file('power2_model.zip', '/kaggle/working/power2_model.zip')\ncopy_file('power3_model.zip', '/kaggle/working/power3_model.zip')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-03T18:41:35.634209Z","iopub.status.idle":"2024-08-03T18:41:35.634536Z","shell.execute_reply.started":"2024-08-03T18:41:35.634370Z","shell.execute_reply":"2024-08-03T18:41:35.634383Z"},"trusted":true},"execution_count":null,"outputs":[]}]}